\chapter[Neural Networks]{Neural Networks}

\section{Biological Neural Networks}

The human nervous system consists in part of many neural cells that are interconnected and can communicate with each other via electrical pulses.
These cells are called {\it neurons}, and they receive input from other neurons via {\it dendrites}, and they can send a signal to a single other neuron via an {\it axon}.
A neuron can also receive signals in reaction to outside stimuli.
Figure \ref{biological_neuron} is a drawing of a biological neuron. 

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.6]{images/biological_neuron}
  \caption{Biological Neuron}
  \label{biological_neuron}
\end{figure}

A network of neurons stores information at the contact points between the neurons.
These contact points are called {\it synapses} and provide a biological neural network with a memory.
Figure \ref{synapses} shows the synapses of a connection between two neurons.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.6]{images/synapses}
  \caption{Synapses}
  \label{synapses}
\end{figure}

Information is stored at the synapses, and the information for a particular neuron synapse can be viewed as a function $f$ of the sum of the weighted edges that are input to the neuron.
The edges in this case are the one or more axons connected to the neuron and the weighted values correspond the efficiency of the particular edge. 
When a neuron receives a signal from a group of axons, it will determine whether or not to send an output signal by applying $f$ to the sum of the weighted value associated to each input axon.
If the neuron does send a signal, this is known as {\it firing} or {\it excitation}.
The efficiency of an edge can be changed through various biological mechanisms.
The changing of the efficiencies of the edges over time can be viewed as learning.
This oversimplification of biological neural networks is sufficient to draw parallels to the theory for artificial neural networks.


\section{Artifical Neural Networks}
An artifical neural network or {\it ANN} is a way of processing
information that is loosely based on the way the nervous system in a
human brain functions.
Neural networks have been used as solutions in pattern recognition problems such as optical character recognition (OCR)\cite{ocr1,ocr2}, facial recognition\cite{face} as well as decision making problems\cite{decisionMaking1,decisionMaking2}.
The purpose of an artificial neural network is to provide a mapping from a set of input data to output data. 
In mathematical terms the goal is to map an n-dimensional real input
$(x1,x2,...,xn)$ to an m-dimensional real output $(y1,y2,...,ym)$, 
approximating a function: \begin{displaymath} F : R^n \rightarrow R^m \cite{rojas1}\end{displaymath}. 
A neural network builds this mapping in an iterative fashion from training data consisting of known input/output pairs that are presented to it. 
The training inputs are a subset of the total possible inputs to the network, and assuming there is a function of input to output data, a neural network can be viewed as a black box that may approximate that function for us.
Henceforth, the term neural network will be used for brevity, with the implicit assumption being that an artificial neural network is meant unless otherwise stated.

As with biological neural networks, the basic building blocks of artificial neural networks are neurons. 
Each neuron is connected to one or more others neurons in the network. 
Each neuron outputs a single value based on the evaluation of a function $f$ of the sum of the values of the inputs to the neuron.
This function is known as an activation function.
Typically the activation function is sigmoidal and it also known as a sqaushing function, since it has the ability to squashes all output between two real values\cite{mitchell1997}.
%TODO They are also differentiable,
The most commonly used sigmoidal function in neural networks is the logisitic function, defined as:
$P(t) = \frac{1}{1 + e^-t}$
Figure \ref{logistic} shows the graph of the function a cartesian coordinate system.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.6]{images/logistic}
  \caption{Logistic function}
  \label{logistic}
\end{figure}

The output value of a neuron with unweighted edges is a function of
its inputs, $f(x_1, x_2, ..., x_n)$ where $x_i$ is the $i^{th}$ of $n$
inputs is shown in Figure \ref{neuron}.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.75]{images/neuron_fn}
  \caption{Artificial neuron}
  \label{neuron}
\end{figure}

The output value of a neuron with weighted edges is a function of its
inputs and weight values of its input edges, $f(x_1 * w_1, x_2 * w_2,
..., x_n * w_n)$ where $x_i$ is the $i^{th}$ of $n$ inputs and $w_i$
is the $i_{th}$ of the corresponding $n$ input edges  is shown in
Figure \ref{weighted_neuron}.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.8]{images/weighted_neuron_fn}
  \caption{Weighted neuron}
  \label{weighted_neuron}
\end{figure}

Here we only consider neural networks with weighted edges. 
In general they are more computationally powerful than neural networks with unweighted edges.
The weighted edges connecting the neurons are simply called weights, and the values associated with these weights can be similarly be adapted as they are with biological neural networks. 
In this way a neural network can learn how to process the information presented to it based on past experience. 
Unlike biological neural networks, we impose a restriction on the topology of the connections.
A network is broken into layers, with one or more neurons belonging to a particular layer.
The restriction depends on the type of neural network.

\subsection{Perceptron}
A simple example of a neural network is one with a single neuron known as a perceptron.
The perceptron takes a vector of values as inputs and outputs a single value, $0$ or $1$:

\[
  f(x) = \left\{ 
  \begin{array}{l l}
    1 & \quad \text{if $w \cdot x > 0$}\\
    0 & \quad \text{else}\\
  \end{array} \right.
\]

A perceptron is trained to learn by adjusting its weights after an
input/output pair is presented to it.

Algorithm \ref{perceptronAlg} describes an algorithm for perceptron training.

\begin{algorithm}[h!]
\caption{Perceptron training algorithm}
\label{perceptronAlg}    
\begin{algorithmic}

\WHILE {all input/output pairs have not been classified correctly}
\STATE Select a pair of training data, input $x_n$ and known output $y_n$
\STATE Present the vector to the perceptron and compute $z$ = $f(x_1 * w_1, x_2 * w_2, ..., x_n * w_n)$
\IF{$z <= 0$ \AND $y_n > 0$}
\STATE $w_{i+1} = w_i + x_n$
\ELSIF{$z > 0$ \AND $y_n <= 0$}
\STATE $w_{i+1} = w_i - x_n$
 \ELSE
\STATE no adjustment
\ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}

Although the perceptron can be trained to classify patterns, there are sets of data that can be separated into two classes that a perceptron is not capable of learning to classify.
A perceptron can only classify data sets that are linearly separable.
The logic operators {\it AND}, {\it OR} and {\it XOR} provide a simple way of visualizing linear separability.
Consider the {\it AND} operator with the truth table in Figure \ref{andtt}.
\begin{table}[h!]
  \caption{AND truth table}
  \label{andtt}
\begin{displaymath}
\begin{array}{|c|c|c|}
   X
 & Y
 & X \land Y
\\
\hline
0 & 0 & 0\\
0 & 1 & 0\\
1 & 0 & 0\\
1 & 1 & 1\\
\hline
\end{array}
\end{displaymath}
\end{table}

Figure \ref{ands} shows the separation of input space for the {\it AND} operator.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.4]{images/and}
  \caption{AND input space separation}
  \label{ands}
\end{figure}

Now consider the {\it OR} operator with the in Figure \ref{ortt}. 

\begin{table}[h!]
\caption{OR truth table}
\label{ortt}
\begin{displaymath}
\begin{array}{|c|c|c|}
   X
 & Y
 & X \lor Y
\\
\hline
0 & 0 & 0\\
0 & 1 & 1\\
1 & 0 & 1\\
1 & 1 & 1\\
\hline
\end{array}
\end{displaymath}
\end{table}

Figure \ref{ors} shows the separation of input space for the {\it OR} operator.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.4]{images/or}
  \caption{OR input space separation}
  \label{ors}
\end{figure}

Now consider the the {\it XOR} operator with the truth table in Figure \ref{xortt}.
\begin{table}[h!]
\caption{XOR truth table}
\label{xortt}
\begin{displaymath}
\begin{array}{|c|c|c|}
   X
 & Y
 & X \oplus Y
\\
\hline
0 & 0 & 0\\
0 & 1 & 1\\
1 & 0 & 1\\
1 & 1 & 0\\
\hline
\end{array}
\end{displaymath}
\end{table}

In Figure \ref{xors} Notice how there is no way to draw a straight line to distinguish between the positive and negative classes.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.4]{images/xor}
  \caption{XOR input space}
  \label{xors}
\end{figure}

Figure \ref{xorss} shows that at least two lines must be used to separate the positive and negative classes.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.4]{images/xor_2}
  \caption{XOR input space separation}
  \label{xorss}
\end{figure}

If a perceptron is given a problem whose input space is linearly
separable, it is guaranteed to find a solution in a finite number of
steps\cite{patternRecognition1}. 
In the worst case the number of iteration steps can grow
exponentially\cite{rojas2}.
There are better algorithms for finding solutions to linearly separable problems in
polynomial time such as Karmarkar's polynomial time
algorithm\cite{karmarkar}.

\subsection{Feed Forward Neural Networks} 
One way to solve a problem with a non-linearly separable input space
is with a feed forward neural network.
With feed forward neural networks, sets of neurons are grouped into
layers and information flows from the input layer to output layer in one direction only.
A neuron in layer $n$ may only send signals to neurons in layer $n+1$, and it may only receive signals from neurons in layer $n-1$.
The $0^{th}$ layer does not receive signals from other neurons and the output of the $n^{th}$ layer can be viewed as the output of the network.
Figure \ref{ff} shows an example of this topology:

\begin{figure}[h!]
  \centering
  \includegraphics{images/feed_forward}
  \caption{Feed forward network topology}
  \label{ff}
\end{figure}

Notice that the edges are directed and indicate that the flow of information is from left to right and that there are no cycles.
A network is feed forward if the connections between its neurons do not form a cycle.

Revisiting the {\it XOR} problem we want to label the input spaces
and create a mapping of each input vector to a single label.
Figure \ref{xor_label} we label the three regions $A$, $B$ and $C$.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.4]{images/xor_labeling}
  \caption{Labeled three regions of the XOR input space}
  \label{xor_label}
\end{figure}

And we want the network to remember which input vector $x$ gets mapped to a
particular label, seen in Figure \ref{xorss_map}.

\begin{table}[h!]
\caption{Mapping of XOR input space to label}
\label{xorss_map}
\begin{displaymath}
\begin{array}{|c|c|c|}
   x_1
 & x_2
 & label
\\
\hline
0 & 0 & A\\
0 & 1 & B\\
1 & 0 & B\\
1 & 1 & C\\
\hline
\end{array}
\end{displaymath}
\end{table}

Figure \ref{xff} is a neural network with a feed forward architecture
solution to the XOR problem.

\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.4]{images/xor_feed_forward}
  \caption{Weighted feed forward neural network that solves the XOR
    problem}
  \label{xff}
\end{figure}

The neural network has three layers, the input layer, one middle
layer, and the output layer.

Feed forward networks provide a powerful computational model capable
of correctly mapping a set of input vectors to a specific output, even
for non-linearly separable input spaces.
However, finding a set of weights that solves a problem
problem becomes increasingly expensive as the arity of the feature
vector expands, the number of nodes increases, or the number of layers
increases.
In addition, traditional methods of solving systems of equations do not work
well with noisy data and missing or incomplete data, as is the case in most real world problems.
To solve these issues, the backprogation algorithm can be used.

\subsection{Backpropagation}

The backpropagation algorithm is the most popular algorithm for
finding a set of weights for a neural network to solve a particular
problem.
It uses a numerical method known as {\it gradient descent} to search
for the minimum of the error function in weight space.

 




In this chapter we discuss a popular learning method capable of handling such large learning problems — the backpropagation algorithm. This numerical method was used by different research communities in different contexts, was discovered and rediscovered, until in 1985 it found its way into connectionist AI mainly through the work of the PDP group [382]. It has been one of the most studied and used algorithms for neural networks learning ever since.





One effectiveness measure of a neural network can be found by calculating it's error rate. A low error rate means the network is good at estimating the problem/solution function. Finding a network with a low error rate for a particular problem is challenging. One of the challenges comes from choosing which features of data to use to form inputs of a training set and how to represent this data. Another challenge is choosing how many hidden layers the network will have, how many nodes per hidden layer should be used, and what the activation function should be used in each hidden layer. The combination of these three things determines the network structure. 



