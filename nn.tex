\chapter[Neural Networks]{Neural Networks}

\section{Biological Neural Networks}

The human nervous system consists in part of many neural cells that are interconnected and can communicate with each other via electrical pulses.
These cells are called {\it neurons}, and they receive input from other neurons via {\it dendrites}, and they can send a signal to a single other neuron via an {\it axon}.
A neuron can also receive signals in reaction to outside stimuli.
The following is a drawing of a biological neuron: 

\includegraphics[scale=0.6]{images/biological_neuron}

A network of neurons stores information at the contact points between the neurons.
These contact points are called {\it synapses} and provide a biological neural network with a memory.

The following shows the synapses of a connection between two neurons:

\includegraphics[scale=0.6]{images/synapses}

Information is stored at the synapses, and the information for a particular neuron synapse can be viewed as a function $f$ of the sum of the weighted edges that are input to the neuron.
The edges in this case are the one or more axons connected to the neuron and the weighted values correspond the efficiency of the particular edge. 
When a neuron receives a signal from a group of axons, it will determine whether or not to send an output signal by applying $f$ to the sum of the weighted value associated to each input axon.
If the neuron does send a signal, this is known as {\it firing} or {\it excitation}.
The efficiency of an edge can be changed through various biological mechanisms.
The changing of the efficiencies of the edges over time can be viewed as learning.
This oversimplification of biological neural networks is sufficient to draw parallels to the theory for artificial neural networks.


\section{Artifical Neural Networks}
An artifical neural network or {\it ANN} is a way of processing information that is loosely based on the way the nervous system in a human brain functions.
The purpose of an artificial neural network is to provide a mapping from a set of input data to output data. 
In mathematical terms the goal is to map an n-dimensional real input
$(x1,x2,...,xn)$ to an m-dimensional real output $(y1,y2,...,ym)$, 
approximating a function: \begin{displaymath} F : R^n \rightarrow R^m \cite{rojas1}\end{displaymath}. 
A neural network builds this mapping in an iterative fashion from training data consisting of known input/output pairs that are presented to it. 
The training inputs are a subset of the total possible inputs to the network, and assuming there is a function of input to output data, a neural network can be viewed as a black box that may approximate that function for us.
Henceforth, the term neural network will be used for brevity, with the implicit assumption being that an artificial neural network is meant unless otherwise stated.

As with biological neural networks, the basic building blocks of artificial neural networks are neurons. 
Each neuron is connected to one or more others neurons in the network. 
Each neuron outputs a single value based on the evaluation of a function $f$ of the sum of the values of the inputs to the neuron.
This function is known as an activation function.
Typically the activation function is sigmoidal and it also known as a sqaushing function, since it has the ability to squashes all output between two real values\cite{mitchell1997}.
%TODO They are also differentiable,
The most commonly used sigmoidal function in neural networks is the logisitic function, defined as:
\begin{displaymath}P(t) = \frac{1}{1 + e^-t}\end{displaymath}

The following shows the graph of the function a cartesian coordinate system:

\includegraphics[scale=0.75]{images/logistic}

The output value of a neuron with unweighted edges is a function of its inputs, $f(x_1, x_2, ..., x_n)$ where $x_i$ is the $i^{th}$ of $n$ inputs:

\includegraphics[scale=0.75]{images/neuron_fn}

The output value of a neuron with weighted edges is a function of its inputs and weight values of its input edges, $f(x_1 * w_1, x_2 * w_2, ..., x_n * w_n)$ where $x_i$ is the $i^{th}$ of $n$ inputs and $w_i$ is the $i_{th}$ of the corresponding $n$ input edges:

\includegraphics[scale=0.75]{images/weighted_neuron_fn}

Here we only consider neural networks with weighted edges. 
In general they are more computationally powerful than neural networks with unweighted edges.
The weighted edges connecting the neurons are simply called weights, and the values associated with these weights can be similarly be adapted as they are with biological neural networks. 
In this way a neural network can learn how to process the information presented to it based on past experience. 
Unlike biological neural networks, we impose a restriction on the topology of the connections.
A network is broken into layers, with one or more neurons belonging to a particular layer.
The restriction depends on the type of neural network.

\subsection{Feed Forward Neural Networks} 
With feed forward neural networks, the information flows in one direction only.
A neuron in layer $n$ may only send signals to neurons in layer $n+1$, and it may only receive signals from neurons in layer $n-1$.
The $0^{th}$ layer does not receive signals from other neurons and the output of the $n^{th}$ layer can be viewed as the output of the network.
The following shows an example of this topology:

\includegraphics{images/feed_forward}

Notice that the edges are directed and indicate that the flow of information is from left to right and that there are no cycles.

\subsubsection{Perceptron}
A simple example of a feed forward neural network is one with a single neuron known as a perceptron.
The perceptron takes a vector of values as inputs and outputs a single value. 
The following is an example of a perceptron: 

%TODO 

The perceptron is known as a feed-forward neural network since no input to the neuron is ever affected by the output of any other neuron. In general, a network is feed-forward if the connections between its neurons do not form a cycle, and all information flows in one direction from left to right.


The following is a basic neural network: 

%TODO



Neural networks have been used as solutions in pattern recognition problems such as optical character recognition (OCR)\cite{ocr1,ocr2}, facial recognition\cite{face} as well as decision making problems\cite{decisionMaking1,decisionMaking2}.






One effectiveness measure of a neural network can be found by calculating it's error rate. A low error rate means the network is good at estimating the problem/solution function. Finding a network with a low error rate for a particular problem is challenging. One of the challenges comes from choosing which features of data to use to form inputs of a training set and how to represent this data. Another challenge is choosing how many hidden layers the network will have, how many nodes per hidden layer should be used, and what the activation function should be used in each hidden layer. The combination of these three things determines the network structure. 



