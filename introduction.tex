\chapter[Introduction]{Introduction}
Certain problems lend themselves toward solutions that are probabilistic in nature and not always deterministic.
These include facial recognition, voice recognition, image recognition, and clustering. 
An artificital neural network ``ANN'' or ``NN'' is one way to solve problems of this nature. From a high level, a neural network is a black
box that given an input pattern, produces an output pattern that is the network's best guess to be correct. 
It essentially estimates a function whose domain is the set of all input patterns and range is the set of all output patterns. Typical NN implementations train an NN for a specific task, one at a time, sequentially. Preprocessing of the data for selecting an appropriate feature vector is tuned, as is selecting appropriate training and test data sets that give low error rates. The tuning of the structure of the NN itself is often ignored. The structure of the NN, including the activation functions at each hidden node, the number of nodes per hidden layer, the number of hidden layers, the constant that defines step length correction at each step, and the constant that defines the momentum factor for help in preventing oscillation during learning may all be tuned to get a lower error rate than other structures that were trained for the same amount of iterations with the same training set.

{\em NNGenerator} is the software used to train multiple networks at a time and combine the results to generate a network that will attempt to adapt to the training set. This software hopes to mitigate the problem of choosing a neural network structure by using an iterative search algorithm similar to a genetic algorithm to find an optimal structure of a neural network for the features of the training set. 

A problem is described to the software as a training set, which is simple a map of inputs, a vector of features, to actual output, a member of the solution space.
Three problems are described in this paper that were used to test the effectiveness of tuning the structure of an NN and not focus so much on the input/output space. The first is the problem of recognizing the XOR pattern commonly found in digital circuits. This is a simple example of a non-linear separable pattern. This problem was also used when constructing the {\em NNGenerator}, to test the functionality of various aspects of the software. The second problem is learning to play a common casino card game, blackjack. This problem is an example of training agents in games based on past events. The agent can learn to get better at a specific task or tasks, and can perform better than a hard-coded heuristic that does not adapt to events in the game. The third problem is to recognize a test set of handwritten characters based on features extracted from binary images of a disparate training set of handwritten characters. This problem is an example of pattern recognition that neural networks are often used in.

With every problem set, we hope to generate a neural network whose structure is demonstratively better than other structures trained with the same data.
