\chapter[OCR Trainer]{OCR Trainer}
Another one of the problems used to test the program with is an
optical character recognition problem, a technique used for converting
handwritten text to a digital format. 
In this case the characters to be recognized are the arithmetic numerals $0,1,2,3,4,5,6,7,8,9$.

The training and data sets are from the MNIST database\cite{mnist}.
The characters used for training the classifier are a set of a subset of 60,000 samples of handwritten numerals of 30,000 patterns from SD-3, and 30,000 patterns from SD-1 from the NIST character set. 
The characters used for training the classifier are a set of a subset of 10,000 samples of handwritten numerals of 5,000 patterns from SD-3, and 5,000 patterns from SD-1 from the NIST character set. 
The intersection of the training set and test set is null as they are disjoint.

The training and test sets are comprised of 2 files each, a binary file containing the image data for the numerals, and a label file containing the correct output of the image to a digital format. 
The test image file is 45MB and so to save time in the feature extraction step, the input/output map for the set is preprocessed once and stored as a Clojure data structure. 
For training, the 45 MB image file and 60K label file is represented as a 5.6 MB input/output file. 
The input consists of extracting a binary string of length 16 that represents a threshold of pixel counts in the input image. 
The string is constructed of taking a 4x4 piece of the image row-wise. 
If the average pixel value is greater than a certain threshold, the value is 1, otherwise the value is 0. 
The output is a string of length 4 that represents the digital numeral as a binary string. 
The values 10,11,12,13,14,15 are never fed to the neural network during training.

When a slave starts the OCR trainer, it first reads the training file from disk and deserializes it into a Clojure data structure that the backpropagation function uses to train a neural network. 
Once this structure is loaded into memory, training begins. 
After training for the specified number of iterations, the slave posts a message to the master containing the results. 
If the master posts a message back, it will be another train OCR
message and the slave will repeat the process. 
If not, the master has finished breeding and will display the resultant neural network structure. 
This structure can be saved and used to test the neural network
against the test data set.

A single network is be generated by presenting the application with a training set, then a test data set that is disjoint from the training set will be used to test the accuracy of the network.


\section{Results}  
\subsection{Result Set 1}

\begin{center}
    \begin{longtable}{ | l | l | l |}
      \caption{OCR Trainer Test Results 1} \label{ocr1} \\
    \hline
  Generation & Lowest RMS Error & Average RMS Error \\ \hline
1 &	4.3502045626631517E-14 &	0.016356935630774453 \\ \hline
2 &	2.0285182716361427E-8 &	0.010798833096735282 \\ \hline
3 &	5.016625900942483E-12 &	0.009313208954575207 \\ \hline
4 &	9.654223812029E-12 &	0.009764548655503679 \\ \hline
5 &	8.822783538883067E-12 &	0.007621033973619068 \\ \hline
6 &	6.432802850701297E-6 &	0.007046347953596382 \\ \hline
7 &	5.3697890904199377E-14 &	0.013685965230928254 \\ \hline
8 &	3.949459206373761E-10 &	0.00832293977963215 \\ \hline
9 &	1.9570865437930458E-24 &	0.0077764095579861804 \\ \hline
10 &	2.506513021332053E-6 &	0.010869428910938178 \\ \hline
\end{longtable}
\end{center}

Table \ref{ocr1} shows the results of running the {\it NNGenerator} software with the following parameters:

\begin{center}
\includegraphics[scale=0.7]{images/oparams_1}
\end{center}

A screenshot of the resultant network is shown in Figure \ref{oresults_1}.

\begin{figure}[hbt!]
  \centering
  \includegraphics[scale=0.3]{images/oresults_1}
  \caption{Resultant neural network for first OCR result set}
  \label{oresults_1}
\end{figure}

The resultant neural network was unable to converge and gave the
following results when run against the test set and only classifed
1,135 of the 10,000 test characters correctly.

\subsection{Result Set 2}

\begin{center}
    \begin{longtable}{ | l | l | l |}
      \caption{OCR Trainer Test Results 2} \label{ocr2} \\
    \hline
  Generation & Lowest RMS Error & Average RMS Error \\ \hline
1 &	7.891880999221867E-8 &	0.0173904761395515 \\ \hline
2 &	2.013460555515104E-4 &	0.013774106331458341 \\ \hline
3 &	1.277980970409587E-4 &	0.012982988605805981 \\ \hline
4 &	1.2173258203796399E-5 &	0.00992557079845578 \\ \hline
5 &	1.1493958742463188E-13 &	0.009978878509184833 \\ \hline
6 &	3.4014167363083914E-12 &	0.00936778989360004 \\ \hline
7 &	7.754248857947593E-9 &	0.01387680402589895 \\ \hline
8 &	1.2555831561710939E-8 &	0.012832542111672472 \\ \hline
9 &	4.3633615514969336E-8 &	0.009243350832757954 \\ \hline
10 &	1.1771243219151047E-15 &	0.008222115616230612 \\ \hline
11 &	4.176121633494215E-15 &	0.009546788434460465 \\ \hline
12 &	1.2740827726504187E-14 &	0.008903159909956734 \\ \hline
13 &	4.237583140121324E-15 &	0.01073538313750781 \\ \hline
14 &	4.109851463789817E-15 &	0.008514012583273027 \\ \hline
15 &	1.9732404636638029E-16 &	0.004604095778052103 \\ \hline
16 &	4.117632915636444E-9 &	0.008408504165641664 \\ \hline
17 &	6.476978089350002E-18 &	0.007001315851873932 \\ \hline
18 &	6.292739877595802E-11 &	0.010569325971334793 \\ \hline
19 &	1.5056421055145552E-15 &	0.007515515537989361 \\ \hline
20 &	5.193350484983424E-18 &	0.009144290171367774 \\ \hline
\end{longtable}
\end{center}

Table \ref{ocr2} shows the results of running the {\it NNGenerator} software with the following parameters:

\begin{center}
\includegraphics[scale=0.7]{images/oparams_2}
\end{center}

A screenshot of the resultant network is shown in Figure \ref{oresults_2}.

\begin{figure}[hbt!]
  \centering
  \includegraphics[scale=0.3]{images/oresults_2}
  \caption{Resultant neural network for second OCR result set}
  \label{oresults_2}
\end{figure}

The resultant neural network was able to classify 3,980 of the 10,000
test images correctly.

\subsection{Result Set 3}

\begin{center}
    \begin{longtable}{ | l | l | l |}
      \caption{OCR Trainer Test Results 3} \label{ocr3} \\
    \hline
  Generation & Lowest RMS Error & Average RMS Error \\ \hline
1 &	7.989291092433633E-5 &	0.019821599105480103 \\ \hline
2 &	4.004050993350956E-11 &	0.00781206556488213 \\ \hline
3 &	2.953372921182323E-8 &	0.011670288713386441 \\ \hline
4 &	3.561004565825299E-10 &	0.013758225968169701 \\ \hline
5 &	4.969952505241766E-12 &	0.012584144555919543 \\ \hline
6 &	9.620313006394983E-13 &	0.010457804379950851 \\ \hline
7 &	1.3588012614107163E-8 &	0.01021952393825524 \\ \hline
8 &	3.1156755853397372E-12 &	0.00900057086660724 \\ \hline
9 &	2.900200837302653E-13 &	0.010918656380408584 \\ \hline
10 &	1.0123949301175608E-12 &	0.008882579007562777 \\ \hline
11 &	3.272790896905573E-11 &	0.009509713284177332 \\ \hline
12 &	2.809366733938291E-11 &	0.009178001985773981 \\ \hline
13 &	1.6355062166394745E-6 &	0.01386270936376654 \\ \hline
14 &	6.541201436743673E-14 &	0.009129671062543086 \\ \hline
15 &	2.3385156478767894E-11 &	0.012915654815698315 \\ \hline
16 &	1.9655423855241817E-11 &	0.010588685426136333 \\ \hline
17 &	1.6125954810677866E-13 &	0.009769752475599788 \\ \hline
18 &	7.890250832783692E-15 &	0.0092041872140139 \\ \hline
19 &	3.2510336758984747E-12 &	0.00768497129908572 \\ \hline
20 &	1.561943265362401E-15 &	0.0086760219170695 \\ \hline
\end{longtable}
\end{center}

Table \ref{ocr3} shows the results of running the {\it NNGenerator} software with the following parameters:

\begin{center}
\includegraphics[scale=0.7]{images/oparams_3}
\end{center}

A screenshot of the resultant network is shown in Figure \ref{oresults_3}.

\begin{figure}[hbt!]
  \centering
  \includegraphics[scale=0.3]{images/oresults_3}
  \caption{Resultant neural network for third OCR result set}
  \label{oresults_2}
\end{figure}

The resultant neural network was able to classify 3,135 of the 10,000
test images correctly.

\section{Evaluation}
All three test runs gave admittedly poor results, with the third (and
best) neural network only able to classify about $40\%$ of the test
characters correctly.
This is probably due to a poor selection of features for recognizing
the characters.
The RMS error did get better over time for the second and third result
sets.
For the first result set, the RMS error over each generation indicates
no trend in either direction.
