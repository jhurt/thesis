\section{Implementation}
When the program starts, a user interface is provided that allows for controlling some parameters of the application. One of these is number of slaves, which does not have to match the number of actual network slaves, it's the number of networks to train at each step. So if you have only eight slaves and you enter sixteen, then each slave will train two neural networks at each iteration of the algorithm. Another parameter is the number of generations to train. As you increase this parameter the time to complete the algorithm increases linearly. One of the hopes of this project is that the neural networks will get progressively better during each iteration. Another parameter sets the amount iterations a slave should train a single network. This is a constant for all slaves so that each generated neural network has been given a fair chance at training it's weights. The remaning parameters put upper bounds on the neural network structures themselves. A maximum number of hidden layers parameter sets an upper bound of the number of hidden layers for a neural network. A maximum number of nodes per layer parameter sets an upper bound on the number of nodes for a hidden layer, the number of input and output nodes depends on the map of input vectors to outputs used during training and cannot be changed by the user. Tweaking the number of iterations to train a network, the number of nodes per layer and number of layers can have a substantial effect on the time it takes for the algorithm to complete. To get the output of each hidden layer and the output layer is on the order of N*M or the multiplication between the weight matrix between the previous layer and the current layer and the nodes in that layer. As the number of nodes increases, the time to complete the operation for a layer increases.

Although the concurrency features of Clojure are nice, they are limited in that they only provide support for shared memory concurrency on a single JVM. The distributed architecture uses a publish/subscribe model that is facilitated via the Java Messaging Service or JMS library~\cite{jms}. JMS provides message brokering for Java applications on a network. The provider of the brokering services do not have to be the same, they just have to implement the JMS specification for a broker. During testing and while collecting results this application uses a message broker provided by Apache ActiveMQ~\cite{activeMQ} but it could have just as easily used another JMS broker. The ActiveMQ broker listens on a single TCP socket for messages from other nodes on the network. Nodes never have to talk directly to each other on the network, they only need to be able to connect to the broker. This allows the master and the slaves to operate on different networks and they never have to communicate directly.

The messaging model is simple, there are two message queues, one for the single master node to write to and for the slaves to read to, and one for the entire set of slaves to write to and for the master to read from. As soon as a master or slave node sends a message, it does not block waiting for a response. An event listener is wired up for both the slaves and master for when messages are received. Clojure multimethods are used to process the messages based on the type of message. Slaves receive a different message type for each problem. The type allows the slave to select the training data set to use for training the network. For example, the XOR input output map will be selected when it receives a TRAIN XOR message. The training sets are distributed along with the slave jars to keep the master from having to send the data set, which can get really large in the case of training images and sound. The master receives a message for each neural network that a slaves trains. It also receives heartbeat messages from slaves to determine when slaves disconnect or might have ran into some other problem that will prevent them from being able to train a network. The algorithm can continue as long as one slave on the network is still able to receive messages. This is undesirable however as the algorithm is meant to fully exploit the parallelism of genetic algorithms by having a one to one mapping between the number of slaves on the network and the size of the population. 

The neural network structures that are generated and bred are all back propagation neural networks with at least one hidden layer. They are encoded as strings representing serialized maps in Clojure. These strings are relatively small compared to serialized Java objects and are also human readable since they are just Clojure data. The serializing and deserializing functions are also simple compared to what would be needed to serialize a Java object. Here is serialization: 

%#Scheme
%[
(defn serialize [x]
  (binding [*print-dup* true] (pr-str x)))
%]

and here is deserialization:

%#Scheme
%[
(defn deserialize [x]
  (let [r (new PushbackReader (new StringReader x))]
    (read r)))
%]


