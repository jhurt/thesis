\chapter[Architecture]{Architecture}

\section{Master/Slave Pattern}
From a high level, the {\em NNGenerator} software is modeled on the popular master/slave pattern. 
This is a pattern that is meant to be used when breaking up a particular task into many smaller tasks and doing the work in parallel.
A single master is responsible for delegating the work to the slaves and collecting and combining the results into a solution.
These smaller tasks may not necessarily work at the same rate.
This can be due either to external issues outside of the programs control, such as high CPU load, or to the nature of the data or problem at hand.

\subsection{Computing the Mandelbrot Set}
An example of this pattern in action is as a solution to calculating the Mandelbrot set.
The Mandelbrot set consists of a set of points in the complex plane. The boundary of the set forms a fractal.
Take the set of functions $f(x)=x^2 + C$ where $C$ is a constant complex number
and $x \in \mathbb{R}$ 
For a particular complex number $C$, $C$ is a member of the Mandelbrot set if
the recurrence relation generated by repeatedly applying $f$ to increasingly large values of $x$, starting with $x=0$ diverges to infinity. 
A two-dimensional binary image of the Mandelbrot set can be generated in order to visualize the set using the real and imaginary parts of a set of 
complex points on a bounded two-dimensional plane. 
Black pixels will represent complex numbers that are members of the set and white pixlels will represent complex numbers that are not members of the set.
Since we have to evaluate a recurrence for each pixel, the runtime of the generation of an image is $O(rNM)$ where $r$ is a constant representing number of recurrence patterns that are evaluated, $N$ is the width of the image and $M$ is the height of the image.
This algorithm is CPU intensive and grows polynomially as the size of the image grows. 
An interesting property of generating this image is that each pixel
$i,j$ of the image can be calculated {\it independently} of the other pixels.
This is because each complex number $C$ can be tested for membership of the Mandelbrot set regardless of all other complex numbers membership status.
 
Such a problem is called {\it embarassingly parallel} and is a prime candidate for using a master/slave architecture in a shared or distributed memory setting for speedup. 
The master will split the image up into smaller images, and it will give each slave a small image region to calculate. 
The slaves will calculate these smaller images in parallel and send the results back to the master. 
When the master has received all of the smaller images, it will assemble them into the resultant image.

Consider a distributed environment with five nodes, one master node and four slave nodes, and an image of size 256 x 256. 
The algorithm begins with the master assigning the slaves a region of the image to calculate. 
Each slave will get assigned one of 4 regions of size 128 x 128 as shown in Figure \ref{m2s}.

\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.7]{images/master_to_slave}
  \caption{Distribution of image regions from master to slave.}
  \label{m2s}
\end{figure}

Each slave calculates its region of the image and sends the resultant sub-image to the master node as shown in Figure \ref{s2m}.

\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.7]{images/slaves_to_master}
  \caption{Slaves sending fragmented pieces of the complete image region to the master.}
  \label{s2m}
\end{figure}

The master then assembles the resultant sub-images into the resultant image as shown in Figure \ref{resultant_image}. 

\begin{figure}[htb!]
  \centering
  \includegraphics{images/mandelbrot}
  \caption{Master assembles resultant image.}
  \label{resultant_image}
\end{figure}

\subsection{Training Neural Networks}
The {\em NNGenerator} consists of two modules; one of these is the master and runs on a single computer with a display and keyboard so that it can accept user input.
The second of these is the slave module which is run multiple times depending on the environment.
The master and slaves communicate with each other via network sockets, and so the slaves can run in a hybrid distributed environment as one or more instances on a single box with a network connection.
The slaves do not communicate with each other and are not aware of each other's presence.
In fact, there does not have to be more than one slave for the software to function.
In the case of only a single slave, the inherent parallelism of the algorithm is not exploited but a solution will still be found, albeit rather slowly.

Each slave is a trainer of neural networks and the master, as the implementor of the genetic algorithm, is responsible for overseeing and coordinating the search.
The master starts by generating a number of random neural network structures based on the inputs given by the user.
A typical structure may look like Figure \ref{nn_structure}.

\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.6]{images/nn_structure}
  \caption{Structure of a neural network in {\it NNGenerator}.}
  \label{nn_structure}
\end{figure}

This is an example randomly generated structure by the software.
It can be used for a data set with two inputs and one output.
There is one hidden layer with six nodes, the last node of a hidden layer is always a bias node whose value is always equal to 1.0.
There are three input nodes because the last input node is always a bias node.
In general, for an $N \times M$ data set, there will be $N + 1$ input nodes and $M$ output nodes.
The upper bound on the number of generated layers is four in this case, and the upper bound for the number of nodes per layer, except for the input and output layers which are fixed, is five.
Notice the connectivity of the structure, each node in layer $N$ is connected to every other node in layer $N+1$.  

The slave nodes understand how to train these structures against various data sets.
After generating a set of random structures constituting the first
population, the master then puts these structures into messages that
also contains a type field that indicates to a slave which data set to
train the structure with as shown in Figure \ref{training_message}.

\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.7]{images/training_message}
  \caption{Sample XOR training message.}
  \label{training_message}
\end{figure}

It pushes each of these structures to a message queue that holds training messages that have not yet been processed by a slave as shown in figure \ref{message_queue}.

\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.7]{images/message_queue}
  \caption{Training message queue.}
  \label{message_queue}
\end{figure}

The slaves on the network consume messages from the queue in first in, first out fashion as shown in Figure \ref{slaves_consume}.

\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.7]{images/slaves_consume}
  \caption{Slaves consuming training messages.}
  \label{slaves_consume}
\end{figure}

Each slave trains the neural network structure it consumed from the queue starting with a random set of initial weights and a training data set common across all slaves.
Upon finishing training, each slave has computed a weight matrix for the neural network structure and data set.
The final combined structure may look like Figure \ref{nn_combined}. 

\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.7]{images/nn}
  \caption{Neural network generated by {\it NNGenerator} that solves the XOR problem.}
  \label{nn_combined}
\end{figure}

The numbers along each weight indicate the weight value of each input node to its corresponding output node. 

Each member of the population are encoded as Clojure data structures called struct maps. 
These struct maps contain information about the structure and fitness of a trained neural network. 
The specific information is the number of hidden layers, the activation function and derivative of the activation function at each hidden node, the number of nodes at each layer, the connectivity of the nodes, the $\alpha$ and $\gamma$ constants of the network, and the training error of the network. 
The $\alpha$ constant is the momentum factor for help in preventing oscillation during learning. 
The $\gamma$ constant is a learning constant that defines step length of correction at each step of the training.
The slave places the resultant neural network structure along with the other information into a training result message as shown in Figure \ref{result_message}.

\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.7]{images/result_message}
  \caption{Sample {\it NNGenerator} training result message.}
  \label{result_message}
\end{figure}

Afterwards, the slave enqueues the message to a resultant message queue for the master to process as shown in Figure \ref{slaves_publish}.

\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.7]{images/slaves_publish}
  \caption{Training result message queue.}
  \label{slaves_publish}
\end{figure}

The master consumes these messages from the queue in a first in, first out fashion as shown in Figure \ref{master_consume}.

\begin{figure}[htb!]
  \centering
  \includegraphics[scale=0.7]{images/master_consume}
  \caption{Master consuming training result message.}
  \label{master_consume}
\end{figure}

Messages are stored in memory until the master has gathered all of the current populations' resultant messages. 
The size of each population is specified by the user before the algorithm starts.
The master then performs reproduction, crossover, and mutation operations to the network structures to create a new population of structures for the slaves to train.
The master discards the bottom half of the population sorted in order of fitness; so the least fit of the population. 
The remaining results consist of the possible parents for the new generation. 
A number of children are generated from two parents by applying crossover and reproduction to the possible parents using a roulette wheel based selection for each set of parents. 
The probability of selecting a parent is determined based on the fitness of the parent in comparison to the fitness of all other parents. 
This way the fitter of any two parents has a higher probability of getting selection for reproduction and/or breeding.

The process continues in this manner until a certain number of generations have been bred. 
This number is specified by the user. 
The fittest network of the last generation is selected as the neural
network most capable of solving the particular problem.
