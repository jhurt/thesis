\section{Architecture}
The software uses a distributed master-slave architecture where each slave is a trainer and the master is the implementor of the genetic algorithm. The master starts by generating a number of random neural network structures based on upper bounds given by the user. It pushes each of these structures to a message queue that the slaves on the network consume messages from one at a time. Each slave trains the neural network structure it consumed from the queue starting with a random set of initial weights and a training data set common across all slaves. Upon finishing training, each slave places a training result in a separate message queue for the master to consume. This message contains the network structure, the weight matrix, and the error percentage of the network that it trained. The master consumes these messages and once it has gathered all of the current populations' results, it performs reproduction, crossover, and mutation operations to the network structures to create a new population of structures for the slaves to train. The process continues in this manner until a certain number of generations have been bred. This number is specified by the user. The fittest network of the last generation is selected as the neural network most capable of solving the particular problem.  

The population are encoded as data structures in Clojure, struct maps. These maps contain information about the structure and fitness of a trained neural network. The specific information is the number of hidden layers, the activation function and derivative of the activation function at each hidden node, the number of nodes at each layer, the connectivity of the nodes, the alpha and gamma constants of the network, and the training error of the network. The alpha constant is the momentum factor for help in preventing oscillation during learning. The gamma constant is a learning constant that defines step length of correction at each step of the training.

Crossover gets applied to the population as follows:

The neural network structures that are generated and bred are all single or multi-layer back propagation networks.  A single network will be generated by presenting the application with a training set, then a test data set that is disjoint from the training set will be used to test the accuracy of the network.  A number of problems will be used to test the network.  With every problem set, we hope to generate a network that is capable of correctly mapping the test data set with a high degree of accuracy.  At the very least we hope that networks of later generations will be more accurate than networks of previous generations so that with each new generation the networks get better at solving the problem.