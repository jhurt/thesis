\chapter[Architecture]{Architecture}

\section{Master/Slave Pattern}
From a high level, the NNGenerator software is modeled on the popular master/slave pattern. 
This is a pattern that is meant to be used when breaking up a particular task into many smaller tasks and doing the work in parallel.
A single master is responsible for delegating the work to the slaves and collecting and combining the results into a solution.
These smaller tasks may not necessarily work at the same rate.
This can be due either to external issues outside of the programs control, such as high CPU load, or to the nature of the data or problem at hand.

An example of this pattern in action is as a solution to calculating the Mandelbrot set.
The Mandelbrot set consists of a set of points in the complex plane. The boundary of the set forms a fractal.
Take the set of functions \begin{math}f(x)=x^2 + C\end{math} where \begin{math}C\end{math} is a constant complex number
and \begin{math}x \in \mathbb{R}\end{math} 
For a particular complex number \begin{math}C\end{math}, \begin{math}C\end{math} is a member of the Mandelbrot set if
the recurrence relation generated by repeatedly appling \begin{math}f\end{math} to increasingly large values of \begin{math}x\end{math}, starting with \begin{math}x=0\end{math} diverges to infinity. 
A two-dimensional binary image of the Mandelbrot set can be generated in order to visualize the set using the real and imaginary parts of a set of 
complex points on a bounded two-dimensional plane. 
Black pixels will represent complex numbers that are members of the set and white pixlels will represent complex numbers that are not members of the set.
Since we have to evaluate a recurrence for each pixel, the runtime of the generation of an image is \begin{math}O(rNM)\end{math} where \begin{math}r\end{math} is a constant representing number of recurrence patterns that are evaluated, \begin{math}N\end{math} is the width of the image and \begin{math}M\end{math} is the height of the image.
This algorithm is CPU intensive and grows exponentially as the size of the image grows. 
An interesting property of generating this image is that each pixel \begin{math}i,j\end{math} of the image can be calculated independently of the other pixels.
This is because each complex number \begin{math}C\end{math} can be tested for membership of the Mandelbrot set regardless of all other complex numbers membership status.
 
This property makes this algorithm a good candidate for using a master/slave architecture in a shared or distributed memory setting for speedup. 
The master will split the image up into smaller images, and it will give each slave a small image region to calculate. 
The slaves will calculate these smaller images in parallel and send the results back to the master. 
When the master has received all of the smaller images, it will assemble them into the resultant image.

Consider a distributed environment with 5 nodes, one master node and 4 slave nodes. and an image of size 256 x 256. 
The algorithm begins with the master assigning the slaves a region of the image to calculate. 
Each slave will get assigned one of 4 regions of size 128 x 128:

\includegraphics{images/master_to_slave}

\newpage
Each slave calculates its region of the image and sends the resultant sub-image to the master node:

\includegraphics{images/slaves_to_master}
\newpage

The master then assembles the resultant sub-images into the resultant image: 

\includegraphics{images/mandelbrot}

\section{Using the Master/Slave Pattern to Train Neural Networks}
The NNGenerator consists of two modules, one of these is the master and runs on a single computer with a display and keyboard so that it can accept user input.
The second of these is the slave module which is run multiple times depending on the environment.
The master and slaves communicate with each other via netwok sockets and so the slaves can run in a hybrid distributed environment as one or more instances on a single box with a network connection. 
Each slave is a trainer of neural networks and the master, as the implementor of the genetic algorithm, is responsible for overseeing and coordinating the search.
The master starts by generating a number of random neural network structures based on upper bounds given by the user. 
It pushes each of these structures to a message queue that the slaves on the network consume messages from one at a time. 
Each slave trains the neural network structure it consumed from the queue starting with a random set of initial weights and a training data set common across all slaves. Upon finishing training, each slave places a training result in a separate message queue for the master to consume. 
This message contains the network structure, the weight matrix, and the error percentage of the network that it trained. 
The master consumes these messages and once it has gathered all of the current populations' results, it performs reproduction, crossover, and mutation operations to the network structures to create a new population of structures for the slaves to train. 
The process continues in this manner until a certain number of generations have been bred. 
This number is specified by the user. 
The fittest network of the last generation is selected as the neural network most capable of solving the particular problem.  

The population are encoded as data structures in Clojure, struct maps. 
These maps contain information about the structure and fitness of a trained neural network. 
The specific information is the number of hidden layers, the activation function and derivative of the activation function at each hidden node, the number of nodes at each layer, the connectivity of the nodes, the alpha and gamma constants of the network, and the training error of the network. 
The alpha constant is the momentum factor for help in preventing oscillation during learning. 
The gamma constant is a learning constant that defines step length of correction at each step of the training.

The master discards the bottom half of the population sorted in order of fitness, so the least fit of the population. 
The remaining results consist of the possible parents for the new generation. 
A number of children is generated from two parents by applying crossover and reproduction to the possible parents using a roulette wheel based selection for each set of parents. 
The probability of selecting a parent is determined based on the fitness of the parent in comparison to the fitness of all other parents. 
This way the fitter of any two parents has a higher probability of getting selection for reproduction and/or breeding.

A single network will be generated by presenting the application with a training set, then a test data set that is disjoint from the training set will be used to test the accuracy of the network.  
