\section{Conclusion}

Examining the blackjack and OCR result sets leads to some interesting observations.
The first is that the generated neural networks with low RMS errors did not necessarily correspond to neural networks that performed well on the test data sets.
This is partly due to a weakness in the fitness function, since it only uses the RMS error to determine fitness.
It is also due to a poor selection of feature selection, particularly in the OCR trainer.

When determining if the genetic algorithm breeds better neural networks with each successive generation, in the case of the OCR trainer it did.
However, in the case of the blackjack trainer, the RMS errors did not appear to trend in any direction with each successive generation.

The results refute the original hypothesis that a neural network structure that performs well can be generated without focusing on the preprocessing and feature vector selection of an input/output data set.
The very simplistic features pulled in the OCR set were not enough to even get to a classification success rate of $50\%$, much less the usually desired high ninety percent ranges.
The blackjack features were much better in comparison, however the results indicate that there was a limit to how well a neural network could perform in a simulated game with the features, namely around a $38\%$ win rate.
That limit was not able to be exceeded no matter what structure the {\it NNGenerator} software generated.
